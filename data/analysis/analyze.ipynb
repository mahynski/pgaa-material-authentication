{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b88c54-8f31-499c-b233-3ab5893056d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import imblearn\n",
    "import pickle\n",
    "\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pacmap\n",
    "\n",
    "import pychemauth\n",
    "\n",
    "# Reload modules automatically if updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# This will list all the packages you have imported and their versions for reproducibility\n",
    "%load_ext watermark\n",
    "%watermark -t -m -h -v --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_figures = False # Whether or not to override existing figures on disk\n",
    "reprocess = False # Re-process data from disk or use saved results?\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = ['serif']\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman']\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194772c-4b54-4119-8f78-909ac597bc84",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd10846-520a-434c-a9a6-cb1d7e030136",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import pacmap\n",
    "\n",
    "def relabel(name, head='../data/raw/training/'):\n",
    "    \"\"\"This is the way we are choosing to collect these materials into different categories.\"\"\"\n",
    "    raw = ' '.join(name.split(head)[1].split('_'))\n",
    "    \n",
    "    if '1085' in raw:\n",
    "        return 'Lubricating Oil'\n",
    "    elif 'steel' in raw:\n",
    "        return 'Steel'\n",
    "    elif 'carbon' in raw:\n",
    "        return 'Carbon Powder'\n",
    "    elif 'concrete' in raw:\n",
    "        return 'Concrete'\n",
    "    elif 'dolomitic' in raw:\n",
    "        return 'Dolomitic Limestone'\n",
    "    elif 'fuel' in raw:\n",
    "        return 'Fuel Oil'\n",
    "    elif 'glass' in raw:\n",
    "        return 'Forensic Glass'\n",
    "    elif 'graphite' in raw:\n",
    "        return 'Graphite/Urea Mixture'\n",
    "    elif '1632' in raw or '2685' in raw or '2692' in raw or '2693' in raw or '2775' in raw:\n",
    "        return 'Coal and Coke'\n",
    "    elif '2790' in raw or '2791' in raw or 'leaves' in raw:\n",
    "        return 'Biomass'\n",
    "    elif 'phosphate' in raw:\n",
    "        return 'Phosphate Rock'\n",
    "    elif 'titanium' in raw:\n",
    "        return 'Titanium Alloy'\n",
    "    elif 'zircaloy':\n",
    "        return 'Zircaloy'\n",
    "    else:\n",
    "        raise Exception('Unrecognized: '+raw)\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "def pca_project(X, y, ax, title=None, threshold=0.0, ellipses=False, min_d=0.0, scale=1):\n",
    "    # Linear approach\n",
    "    pca = PCA(n_components=2)\n",
    "    vt = VarianceThreshold(threshold=threshold)\n",
    "    ss = StandardScaler()\n",
    "    X_proj = pca.fit_transform(ss.fit_transform(vt.fit_transform(X)))\n",
    "    cycle_colors = plt.cm.turbo(np.linspace(0.01, 0.99, len(np.unique(y))))\n",
    "    \n",
    "    for i,class_ in enumerate(np.unique(y)):\n",
    "        mask = y == class_\n",
    "        ax.plot(X_proj[mask,0], X_proj[mask,1], '.', label=class_, color=cycle_colors[i], alpha=0.3)\n",
    "        if ellipses:\n",
    "            def eigsorted(cov):\n",
    "                vals, vecs = np.linalg.eigh(cov)\n",
    "                order = vals.argsort()[::-1]\n",
    "                return vals[order], vecs[:,order]\n",
    "\n",
    "            xdata = X_proj[mask, 0]\n",
    "            ydata = X_proj[mask, 1]\n",
    "\n",
    "            # Get values to build the ellipse\n",
    "            cov = np.cov(xdata, ydata)\n",
    "            vals, vecs = eigsorted(cov)\n",
    "            theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "            w, h = 1 * 2 * np.sqrt(vals) * scale\n",
    "            \n",
    "            w = np.max([min_d, w]) # For visualization\n",
    "            h = np.max([min_d, h]) # For visualization\n",
    "\n",
    "            # Create the ellipse\n",
    "            ell = Ellipse(xy=(np.mean(xdata), np.mean(ydata)),\n",
    "                  width=w, height=h,\n",
    "                  angle=theta, color='black', alpha=0.2)\n",
    "\n",
    "            ell.set_facecolor(cycle_colors[i]) # Reference the colour for each factor (defined by lab)\n",
    "            ax.add_artist(ell)\n",
    "        \n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    return pca, vt\n",
    "\n",
    "def pacmap_project(X, y, ax, title=None, threshold=0.0, ellipses=False, min_d=0.0, scale=1):\n",
    "    # Non-linear approach\n",
    "    embedding = pacmap.PaCMAP(n_components=2, n_neighbors=5,\n",
    "                          MN_ratio=0.5, FP_ratio=2.0,\n",
    "                          distance='euclidean',\n",
    "                          apply_pca=False, random_state=42)\n",
    "    \n",
    "    vt = VarianceThreshold(threshold=threshold)\n",
    "    ss = StandardScaler()\n",
    "    X_proj = embedding.fit_transform(ss.fit_transform(vt.fit_transform(X)))\n",
    "    cycle_colors = plt.cm.turbo(np.linspace(0.01, 0.99, len(np.unique(y))))\n",
    "    \n",
    "    for i,class_ in enumerate(np.unique(y)):\n",
    "        mask = y == class_\n",
    "        ax.plot(X_proj[mask,0], X_proj[mask,1], 'o', label=class_, color=cycle_colors[i])\n",
    "        if ellipses:\n",
    "            def eigsorted(cov):\n",
    "                vals, vecs = np.linalg.eigh(cov)\n",
    "                order = vals.argsort()[::-1]\n",
    "                return vals[order], vecs[:,order]\n",
    "\n",
    "            xdata = X_proj[mask, 0]\n",
    "            ydata = X_proj[mask, 1]\n",
    "\n",
    "            # Get values to build the ellipse\n",
    "            cov = np.cov(xdata, ydata)\n",
    "            vals, vecs = eigsorted(cov)\n",
    "            theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "            w, h = 2 * 2 * np.sqrt(vals) * scale\n",
    "            \n",
    "            w = np.max([min_d, w]) # For visualization\n",
    "            h = np.max([min_d, h]) # For visualization\n",
    "\n",
    "            # Create the ellipse\n",
    "            ell = Ellipse(xy=(np.mean(xdata), np.mean(ydata)),\n",
    "                  width=w, height=h,\n",
    "                  angle=theta, color='black', alpha=0.2)\n",
    "\n",
    "            ell.set_facecolor(cycle_colors[i]) # Reference the colour for each factor (defined by lab)\n",
    "            ax.add_artist(ell)\n",
    "            \n",
    "    ax.set_xlabel('Dim 1')\n",
    "    ax.set_ylabel('Dim 2')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    return embedding, vt\n",
    "\n",
    "def create_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then generate info for the dendrogram\n",
    "\n",
    "    # Create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # Leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "    \n",
    "    # Plot the corresponding dendrogram\n",
    "    dend = dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "    return dend\n",
    "\n",
    "def smoothsegment(seg, Nsmooth=100):\n",
    "    return np.concatenate([[seg[0]], np.linspace(seg[1], seg[2], Nsmooth), [seg[3]]])\n",
    "\n",
    "def plot_dendrogram(dend, full_labels, y, gap=0.2):\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "\n",
    "    icoord = np.array(dend['icoord']) # x coordinates of dendrogram lines\n",
    "    dcoord = np.array(dend['dcoord']) # y coordinates of dendrogram lines\n",
    "\n",
    "    # For angular, project x coordinates into [0, 2*pi]\n",
    "    ang_coord = ((icoord - icoord.min())/(icoord.max() - icoord.min()))*((1-gap) + gap/2)*2*np.pi\n",
    "\n",
    "    # For radial, project y coordinates into [0.1, 1.0] and take -log\n",
    "    r_coord = -np.log(0.9*((dcoord - dcoord.min()) / (dcoord.max() - dcoord.min())) + 0.1)\n",
    "\n",
    "    for xs, ys in zip(ang_coord, r_coord):\n",
    "        xs = smoothsegment(xs)\n",
    "        ys = smoothsegment(ys)\n",
    "        ax.plot(xs, ys, color=\"silver\")\n",
    "\n",
    "    ax.spines['polar'].set_visible(False)\n",
    "    ax.spines['inner'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    leaf_x = []\n",
    "    exclude_x = []\n",
    "    for xs, ys in zip(icoord, dcoord):\n",
    "        if (ys[0] == 0.0):\n",
    "            leaf_x.append(xs[0])\n",
    "        if (ys[-1] == 0.0):\n",
    "            leaf_x.append(xs[-1])\n",
    "        if np.all(ys == 0.0):\n",
    "            exclude_x.append(np.average([xs[0], xs[-1]]))\n",
    "    \n",
    "    exclude_bool = [l not in exclude_x for l in leaf_x] # Only False where in exclude_bool\n",
    "    leaf_x = np.array(leaf_x)[exclude_bool]\n",
    "    leaf_ang = ((leaf_x - icoord.min())/(icoord.max() - icoord.min()))*((1-gap) + gap/2)*2*np.pi\n",
    "    leaf_sort_inds = np.argsort(leaf_ang)\n",
    "    leaf_ang = leaf_ang[leaf_sort_inds]\n",
    "\n",
    "    # If each point is a node, gives ordering of points in left-to-right traversal of leaves\n",
    "    dend_labels = np.array(full_labels)[dend['leaves']]\n",
    "\n",
    "    # Loop over classes and plot all corresponding points in same color\n",
    "    unique_y = np.unique(y)\n",
    "    class_colors = plt.cm.turbo(np.linspace(0.0, 1.0, len(unique_y)))\n",
    "    for i, class_ in enumerate(unique_y):\n",
    "        this_x = leaf_ang[[(class_ in dl) for dl in dend_labels]]\n",
    "        ax.plot(this_x, r_coord.max()*np.ones_like(this_x),\n",
    "                '.', label=class_, color=class_colors[i], markersize=5.0)\n",
    "\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "    fig.tight_layout()\n",
    "\n",
    "def make_dend(X, y, files):\n",
    "    agg_model = AgglomerativeClustering(n_clusters=None, distance_threshold=0.0, affinity='euclidean', linkage='single')\n",
    "    agg_clust = agg_model.fit(X)\n",
    "    full_labels = [y[i]+'/'+files[i] for i in range(len(y))]\n",
    "    dend = create_dendrogram(agg_model,\n",
    "                         no_plot=True,\n",
    "                         color_threshold=0,\n",
    "                         truncate_mode=None,\n",
    "                         labels=full_labels)\n",
    "    plot_dendrogram(dend, full_labels, y, gap=0.2)\n",
    "    \n",
    "def plot_importance(gs, aligned_centers, dim, return_max=1, ax=None):\n",
    "    scalings = np.abs(gs.best_estimator_.named_steps['dim_red'].components_[dim])\n",
    "    if 'variance_threshold' in gs.best_estimator_.named_steps:\n",
    "        peaks = gs.best_estimator_.named_steps['variance_threshold'].inverse_transform(scalings.reshape(1,-1))[0]\n",
    "    else:\n",
    "        peaks = scalings\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.gca()\n",
    "\n",
    "    ax.plot(\n",
    "        aligned_centers,\n",
    "        peaks,\n",
    "    )\n",
    "    \n",
    "    return peaks[np.argsort(peaks)[::-1]][:return_max], aligned_centers[np.argsort(peaks)[::-1]][:return_max]\n",
    "\n",
    "def visualize_spectra(df, aligned_centers, chosen_cutoff=None):\n",
    "    for label in df['label'].unique():\n",
    "        plt.figure(figsize=(16,4))\n",
    "        y_ = df[df['label'] == label].drop(['label', 'file_names'], axis=1).values\n",
    "        _ = [plt.plot(aligned_centers, a, '.', alpha=0.4) \n",
    "             for a in y_]\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Energy (keV)')\n",
    "        plt.ylabel('Spectra')\n",
    "        plt.title('Original label: '+label+' ; New label: '+relabel(label))\n",
    "        plt.axvline(chosen_cutoff, color='r')\n",
    "        \n",
    "def plot_hard_decision_boundaries_2d(X_proj, y, clf, resolution=0.02, bounds=5):\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(y)\n",
    "    \n",
    "    cmap = plt.cm.tab20\n",
    "    \n",
    "    x1_min, x1_max = X_proj[:,0].min()-bounds*resolution, X_proj[:,0].max()+bounds*resolution\n",
    "    x2_min, x2_max = X_proj[:,1].min()-bounds*resolution, X_proj[:,1].max()+bounds*resolution\n",
    "    \n",
    "    xg1, xg2 = np.meshgrid(\n",
    "        np.arange(x1_min, x1_max, resolution),\n",
    "        np.arange(x2_min, x2_max, resolution),\n",
    "    )\n",
    "    \n",
    "    Z = enc.transform(clf.predict(np.array([xg1.ravel(), xg2.ravel()]).T))\n",
    "    Z = Z.reshape(xg1.shape)\n",
    "    plt.contourf(xg1, xg2, Z, alpha=0.4, levels=len(np.unique(y))+1, cmap='gray')#, cmap=plt.cm.tab20)\n",
    "    \n",
    "    for i,class_ in enumerate(np.unique(y)):\n",
    "        mask = y == class_\n",
    "        color = cmap(i)\n",
    "        plt.plot(X_proj[mask,0], X_proj[mask,1], '.', label=class_, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530a602-1f03-491f-afb3-bf485370124a",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bce8a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "head = '../../data/raw/training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c045e09-a888-4593-8948-379a9819363b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "tags": []
   },
   "source": [
    "## Either Load from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c937c1d-2722-489e-8ea1-3adf04d29e15",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not reprocess:\n",
    "    import pickle\n",
    "    X_use = pickle.load(open('../../data/raw/X_use.pkl', 'rb'))\n",
    "    y_use = pickle.load(open('../../data/raw/y_use.pkl', 'rb'))\n",
    "    files_use = pickle.load(open('../../data/raw/files_use.pkl', 'rb'))\n",
    "    aligned_centers = pickle.load(open('../../data/raw/aligned_centers.pkl', 'rb'))\n",
    "\n",
    "    X_challenge = pickle.load(open('../../data/raw/X_challenge.pkl', 'rb'))\n",
    "    y_challenge = pickle.load(open('../../data/raw/y_challenge.pkl', 'rb'))\n",
    "    files_challenge = pickle.load(open('../../data/raw/files_challenge.pkl', 'rb'))\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(4,3))\n",
    "    for i, class_ in zip([0, 19, 0], ['Titanium Alloy', 'Coal and Coke', 'Concrete']):\n",
    "        plt.plot(aligned_centers, X_use[y_use == class_][i], alpha=0.65, label=class_) \n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Energy (keV)')\n",
    "    plt.ylabel('Normalized Counts')\n",
    "    plt.legend(loc='best')\n",
    "    plt.axvline(7650, color='r', alpha=0.4)\n",
    "\n",
    "    if new_figures:\n",
    "        plt.savefig('example_spectra.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e435a67-aa7e-4b43-8cab-193722c07ce0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "tags": []
   },
   "source": [
    "## Or Re-process Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d026a-43b8-49f2-94c3-30ff19c27b20",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../code/')\n",
    "import load_data, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da81b5-fe75-4ded-abe5-4b43a3d7569b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if reprocess:\n",
    "    # 1. Read raw data\n",
    "    spectra = []\n",
    "    energies = []\n",
    "    y = []\n",
    "    file_names = []\n",
    "\n",
    "    for root, subdirectories, files in os.walk(head):\n",
    "        for file in files:\n",
    "            if 'SPE' not in file:\n",
    "                continue\n",
    "            raw, bins = load_data.read_spe(os.path.join(root, file), \n",
    "                                           coarsen=1, # Coarsen after alignment\n",
    "                                           convert=True, # Convert from bin # to energy (keV)\n",
    "                                           annihilation=False, # Leave annihilation peak\n",
    "                                           normalize=None, # No normalization for now\n",
    "                                          )\n",
    "            spectra.append(raw)\n",
    "            energies.append(bins)\n",
    "            assert(len(subdirectories) == 0)\n",
    "            y.append(root.split(head+'/')[-1])\n",
    "            file_names.append(file)\n",
    "    spectra = np.array(spectra)\n",
    "    energies = np.array(energies)\n",
    "    y = np.array(y)\n",
    "    file_names = np.array(file_names)\n",
    "\n",
    "    # 2. Align energy bins - coarsen and drop the first 40 bins\n",
    "    # Alignment also uses first/last values in spectra to extrapolate instead of to 0\n",
    "    aligned_spectra, aligned_centers = load_data.align_spectra(spectra, energies, coarsen=4, drop=40)\n",
    "\n",
    "    # 3. Create DataFrame\n",
    "    df = pd.DataFrame(data=aligned_spectra)\n",
    "    df['label'] = y\n",
    "    df['file_names'] = file_names\n",
    "    \n",
    "    # Visualize - observe that not all data spans the same range of energies\n",
    "    visualize_spectra(df, aligned_centers, chosen_cutoff=7650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db2f01",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if reprocess:\n",
    "    # Now let's empirically normalize the spectra after coarsening and dropping\n",
    "    df = pd.DataFrame(\n",
    "        data=(aligned_spectra.T / np.sum(aligned_spectra, axis=1)).T\n",
    "    )\n",
    "    df['label'] = y\n",
    "    df['file_names'] = file_names\n",
    "    \n",
    "    visualize_spectra(df, aligned_centers, chosen_cutoff=7650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3f2e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if reprocess:\n",
    "    # Give categories our chosen names\n",
    "    df['pretty label'] = df.label.apply(lambda x:relabel(x))\n",
    "\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.histplot(df.sort_values(by='pretty label'), x='pretty label')\n",
    "    _ = plt.xticks(rotation=90)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Number of Samples')\n",
    "\n",
    "    # Only train on categories with at least 10 observations - rest will be in challenge set\n",
    "    min_obs = 10\n",
    "    plt.axhline(min_obs, color='red')\n",
    "\n",
    "    if new_figures:\n",
    "        plt.savefig('counts.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8341980",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if reprocess:\n",
    "    cats, counts = np.unique(df['pretty label'], return_counts=True)\n",
    "    training_cats = dict([ca,co] for ca,co in zip(cats, counts) if co > min_obs)\n",
    "    training_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd20d83",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if reprocess:\n",
    "    X = df.drop(['label', 'file_names', 'pretty label'], axis=1).values\n",
    "    y = df['pretty label'].values\n",
    "    columns = df.drop(['label', 'file_names', 'pretty label'], axis=1).columns\n",
    "    files = df['file_names'].values\n",
    "    df['pretty label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c24c7-9fd8-4629-a89e-70f518697b8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if reprocess:\n",
    "    mask = np.array([True if c in training_cats else False for c in y])\n",
    "\n",
    "    X_use = X[mask,:]\n",
    "    y_use = y[mask]\n",
    "    files_use = files[mask]\n",
    "\n",
    "    X_challenge = X[~mask,:]\n",
    "    y_challenge = y[~mask]\n",
    "    files_challenge = files[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a40dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if reprocess:\n",
    "#     pickle.dump(X_use, open('../../data/raw/X_use.pkl', 'wb'))\n",
    "#     pickle.dump(y_use, open('../../data/raw/y_use.pkl', 'wb'))\n",
    "#     pickle.dump(files_use, open('../../data/raw/files_use.pkl', 'wb'))\n",
    "#     pickle.dump(aligned_centers, open('../../data/raw/aligned_centers.pkl', 'wb'))\n",
    "\n",
    "#     pickle.dump(X[~mask,:], open('../../data/raw/X_challenge.pkl', 'wb'))\n",
    "#     pickle.dump(y[~mask], open('../../data/raw/y_challenge.pkl', 'wb'))\n",
    "#     pickle.dump(files[~mask], open('../../data/raw/files_challenge.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef36f5-b7d5-476b-a8a8-591e71d29bda",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dimensionality Reduction by Variance Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d1583-0164-48eb-923d-6e66b40eb90b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(aligned_centers, np.std(X_use, axis=0)**2)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Energy (keV)')\n",
    "plt.ylabel(r'$\\sigma^2$')\n",
    "\n",
    "chosen_cutoff = 7650\n",
    "\n",
    "plt.axvline(chosen_cutoff, color='r', alpha=0.25)\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('variance.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea8074",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bin_cutoff = np.argmin((aligned_centers - chosen_cutoff)**2)\n",
    "aligned_centers[bin_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f4cee",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Consider PCA and PaCMAP when using thresholding to eliminate high energy, low variance bins (\"automatically\")\n",
    "thresholds = [0.0, 1.0e-12, 1.0e-10, 1.0e-8, 1.0e-6, 1.0e-4]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=len(thresholds), figsize=(25,10))\n",
    "\n",
    "pca_proj = []\n",
    "pacmap_proj = []\n",
    "for i,thresh in enumerate(thresholds):\n",
    "    pca_, vt_ = pca_project(X_use, y_use, axes[0][i], threshold=thresh, title=\"\")\n",
    "    axes[0][i].set_title('PCA with T={} ({} bins)'.format(thresh, np.sum(vt_.get_support())))\n",
    "    pca_proj.append([pca_, vt_])\n",
    "    \n",
    "    pmp_, vt1_ = pacmap_project(X_use, y_use, axes[1][i], threshold=thresh, title='PaCMAP with T={}'.format(thresh))\n",
    "    axes[1][i].set_title('PaCMAP with T={} ({} bins)'.format(thresh, np.sum(vt1_.get_support())))\n",
    "    pacmap_proj.append([pmp_, vt1_])\n",
    "    \n",
    "axes[0][-1].legend(loc='best', fontsize=8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c2f11-faa9-4442-9f4b-b971d4857408",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(30,18))\n",
    "\n",
    "for i, (ax, vt, pca) in enumerate(zip(axes, [x[1] for x in pca_proj], [x[0] for x in pca_proj])):\n",
    "    ax[0].plot(\n",
    "        aligned_centers, \n",
    "        vt.inverse_transform(np.abs(pca.components_[0]).reshape(1,-1))[0],\n",
    "        color='C0'\n",
    "    )\n",
    "    ax[0].set_xlabel('Energy (keV)', fontsize=20)\n",
    "    ax[0].set_ylabel('|Coeff.| in PC1', fontsize=20)\n",
    "    ax[0].tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    ax[1].plot(\n",
    "        aligned_centers, \n",
    "        vt.inverse_transform(np.abs(pca.components_[1]).reshape(1,-1))[0],\n",
    "        color='C1'\n",
    "    )\n",
    "    ax[1].set_xlabel('Energy (keV)', fontsize=20)\n",
    "    ax[1].set_ylabel('|Coeff.| in PC2', fontsize=20)\n",
    "    ax[1].tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    ax[2].plot(\n",
    "        aligned_centers, \n",
    "        vt.inverse_transform(np.abs(pca.components_[0]).reshape(1,-1))[0],\n",
    "        color='C0',\n",
    "        label='PC 1',\n",
    "        alpha=0.4\n",
    "    )\n",
    "    ax[2].plot(\n",
    "        aligned_centers, \n",
    "        vt.inverse_transform(np.abs(pca.components_[1]).reshape(1,-1))[0],\n",
    "        color='C1',\n",
    "        label='PC 2',\n",
    "        alpha=0.4\n",
    "    )\n",
    "    ax[2].set_xlabel('Energy (keV)', fontsize=20)\n",
    "    ax[2].set_ylabel('|Coeff.|', fontsize=20)\n",
    "    ax[2].legend(loc=0, fontsize=20)\n",
    "    ax[2].tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    ax[0].set_title('Threshold = {} ({} bins)'.format(thresholds[i], np.sum(vt.get_support())), fontsize=28)\n",
    "    ax[1].set_title('Threshold = {} ({} bins)'.format(thresholds[i], np.sum(vt.get_support())), fontsize=28)\n",
    "    ax[2].set_title('Threshold = {} ({} bins)'.format(thresholds[i], np.sum(vt.get_support())), fontsize=28)\n",
    "    \n",
    "    ax[2].axvline(chosen_cutoff, color='k')\n",
    "plt.tight_layout()\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('thresholding.png', dpi=900, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b9263",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Thresholding of about 1e-8 to 1.e-10 is necessary to \"cutoff\" the high energy parts of the spectra we may not trust\n",
    "# since they are not present for all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c76b3",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 14\n",
    "\n",
    "def change_axis_color(ax, color):\n",
    "    ax.spines['bottom'].set_color(color)\n",
    "    ax.spines['top'].set_color(color) \n",
    "    ax.spines['right'].set_color(color)\n",
    "    ax.spines['left'].set_color(color)\n",
    "    \n",
    "fig = plt.figure(constrained_layout=False, facecolor='white', figsize=(18,8))\n",
    "gs = fig.add_gridspec(nrows=5, ncols=4, left=0.05, right=0.75,\n",
    "                      hspace=1.2, wspace=0.3)\n",
    "\n",
    "idx = 0\n",
    "ax0 = fig.add_subplot(gs[0, :2])\n",
    "\n",
    "vt, pca = pca_proj[idx][1], pca_proj[idx][0]\n",
    "ax0.plot(\n",
    "        aligned_centers, \n",
    "        vt.inverse_transform(np.abs(pca.components_[0]).reshape(1,-1))[0],\n",
    "        color='C0',\n",
    "        label='PC 1',\n",
    "        alpha=0.4\n",
    "    )\n",
    "ax0.plot(\n",
    "        aligned_centers, \n",
    "        vt.inverse_transform(np.abs(pca.components_[1]).reshape(1,-1))[0],\n",
    "        color='C1',\n",
    "        label='PC 2',\n",
    "        alpha=0.4\n",
    "    )\n",
    "ax0.set_xlabel('Energy (keV)')\n",
    "ax0.set_ylabel('|Coeff.|')\n",
    "ax0.legend(loc=0, fontsize=12)\n",
    "ax0.set_title('Threshold={} ({} bins)'.format(int(thresholds[idx]), np.sum(vt.get_support())))\n",
    "ax0.axvline(chosen_cutoff, color='k')\n",
    "change_axis_color(ax0, 'red')\n",
    "\n",
    "idx = 3\n",
    "ax0 = fig.add_subplot(gs[0, 2:])\n",
    "\n",
    "vt, pca = pca_proj[idx][1], pca_proj[idx][0]\n",
    "ax0.plot(\n",
    "        aligned_centers, \n",
    "        vt.inverse_transform(np.abs(pca.components_[0]).reshape(1,-1))[0],\n",
    "        color='C0',\n",
    "        label='PC 1',\n",
    "        alpha=0.4\n",
    "    )\n",
    "ax0.plot(\n",
    "        aligned_centers, \n",
    "        vt.inverse_transform(np.abs(pca.components_[1]).reshape(1,-1))[0],\n",
    "        color='C1',\n",
    "        label='PC 2',\n",
    "        alpha=0.4\n",
    "    )\n",
    "ax0.set_xlabel('Energy (keV)')\n",
    "ax0.set_ylabel('|Coeff.|')\n",
    "ax0.legend(loc=0, fontsize=12)\n",
    "ax0.set_title('Threshold={} ({} bins)'.format(thresholds[idx], np.sum(vt.get_support())))\n",
    "ax0.axvline(chosen_cutoff, color='k')\n",
    "change_axis_color(ax0, 'blue')\n",
    "\n",
    "for j,(i,md_pca,md_pacmap) in enumerate(zip([0, 1, 3, 5], [15, 10, 5, 0.5], [5, 5, 5, 5])):\n",
    "    ax_ = fig.add_subplot(gs[1:3, j])\n",
    "    pca_, vt_ = pca_project(X_use, y_use, ax_, threshold=thresholds[i], title=\"\", ellipses=True, min_d=md_pca, scale=2.5)\n",
    "    \n",
    "    \n",
    "    ax_.set_title(r'$\\it{T}$'+'={} '.format(\n",
    "        np.format_float_scientific(thresholds[i]) if i>0 else int(thresholds[i])\n",
    "    )+'({} bins)'.format(np.sum(vt_.get_support())))\n",
    "    \n",
    "    if j == 0:\n",
    "        change_axis_color(ax_, 'red')\n",
    "    elif j == 2:\n",
    "        change_axis_color(ax_, 'blue')\n",
    "    elif j == 3:\n",
    "        ax_.legend(loc='upper right', fontsize=9)\n",
    "        ax_.set_xlim((-5, 12))\n",
    "        ax_.set_ylim((-4, 10))\n",
    "    \n",
    "    ax_ = fig.add_subplot(gs[3:, j])\n",
    "    _, vt_ = pacmap_project(X_use, y_use, ax_, threshold=thresholds[i], title=\"\", \n",
    "                            ellipses=True, min_d=md_pacmap, scale=1.25)\n",
    "\n",
    "mpl.rcParams['font.size'] = 12\n",
    "    \n",
    "if new_figures:\n",
    "    plt.savefig('dim_red.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5611e61-28cd-49f5-a111-7d0aad77f33e",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772eb5b8-7470-4246-8e6a-302808aa1acd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Agglomerative clustering to create dendrograms\n",
    "\n",
    "# Use thresholding to remove high E bins\n",
    "pca = PCA(n_components=2)\n",
    "vt = VarianceThreshold(threshold=1.0e-8) # ~ the same as removing all high E bins\n",
    "ss = StandardScaler()\n",
    "X_proj = pca.fit_transform(ss.fit_transform(vt.fit_transform(X_use)))\n",
    "make_dend(X_proj, [y_ for y_ in y_use], files_use)\n",
    "\n",
    "# Manually trim bins at a chosen cutoff\n",
    "pca = PCA(n_components=2)\n",
    "ss = StandardScaler()\n",
    "X_proj = pca.fit_transform(ss.fit_transform(X_use[:,:bin_cutoff]))\n",
    "make_dend(X_proj, [y_ for y_ in y_use], files_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf38a6-2511-4577-82d9-74b164021ba0",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding = pacmap.PaCMAP(n_components=2, n_neighbors=5,\n",
    "                          MN_ratio=0.5, FP_ratio=2.0,\n",
    "                          distance='euclidean',\n",
    "                          apply_pca=False, random_state=42)\n",
    "vt = VarianceThreshold(threshold=1.0e-8) # ~ the same as removing all high E bins\n",
    "ss = StandardScaler()\n",
    "X_proj = embedding.fit_transform(ss.fit_transform(vt.fit_transform(X_use)))\n",
    "make_dend(X_proj, [y_ for y_ in y_use], files_use)\n",
    "\n",
    "# Manually trim instead of threshold\n",
    "embedding = pacmap.PaCMAP(n_components=2, n_neighbors=5,\n",
    "                          MN_ratio=0.5, FP_ratio=2.0,\n",
    "                          distance='euclidean',\n",
    "                          apply_pca=False, random_state=42)\n",
    "X_proj = embedding.fit_transform(X_use[:,:bin_cutoff])\n",
    "make_dend(X_proj, [y_ for y_ in y_use], files_use)\n",
    "\n",
    "# Without thresholding \n",
    "embedding = pacmap.PaCMAP(n_components=2, n_neighbors=5,\n",
    "                          MN_ratio=0.5, FP_ratio=2.0,\n",
    "                          distance='euclidean',\n",
    "                          apply_pca=False, random_state=42)\n",
    "X_proj = embedding.fit_transform(X_use)\n",
    "make_dend(X_proj, [y_ for y_ in y_use], files_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3ad2a-2114-4c22-bddd-c66bd449ef3f",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# Examine Discriminative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6008f-7495-4e85-88ea-8d906b9db0e1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('model_performances')\n",
    "from pipe_preprocessing import final_fit, preprocessing_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32a21f-ec8b-49d3-9cbe-8e584eef7178",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select a stratified 80:20 split for train:test\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_use, y_use, test_size=0.2, random_state=42, stratify=y_use)\n",
    "\n",
    "# Will do 5-fold CV to optimize hyperparameters\n",
    "k_fold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9eb99-e175-4862-971f-8785d8c8cbbd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Overall Performances from Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1a6d3-b69e-41ff-8eba-d0eed6087b5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "overall = []\n",
    "methods = ['logreg', 'lda', 'qda', 'rf', 'dtree', 'hard_plsda']\n",
    "pipes = [\"_thresh\", \"_thresh_pca\", \"_thresh_standard\", \"_thresh_standard_pca\", \"_thresh_robust\", \"_thresh_robust_pca\"]\n",
    "for method in methods:\n",
    "    results = pickle.load(open('./model_performances/'+method+'_summary.pkl', 'rb'))\n",
    "    column = []\n",
    "    for pipe in pipes:\n",
    "        perf = results[method+pipe]\n",
    "        column += [np.mean(perf), np.std(perf)]\n",
    "    overall.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad281b-2676-47e4-b187-7a65bf58ec03",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def name_methods(m):\n",
    "    if m == 'logreg':\n",
    "        return 'Logistic Regression'\n",
    "    elif m == 'lda':\n",
    "        return 'Linear Discriminant Analysis'\n",
    "    elif m == 'qda':\n",
    "        return 'Quadratic Discriminant Analysis'\n",
    "    elif m == 'rf':\n",
    "        return 'Random Forest'\n",
    "    elif m == 'dtree':\n",
    "        return 'Decision Tree'\n",
    "    elif m == 'hard_plsda':\n",
    "        return 'Hard PLS-DA'\n",
    "    else:\n",
    "        raise ValueError('unknown name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845a4c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 3)\n",
    "df = pd.DataFrame(data=overall, index=[name_methods(m) for m in methods], columns=np.array([[\"<A> ({})\".format(p), \"+/-\"] for p in pipes]).ravel())\n",
    "\n",
    "df['mean'] = df[['<A> (_thresh)', '<A> (_thresh_pca)', '<A> (_thresh_standard)', '<A> (_thresh_standard_pca)', \n",
    "    '<A> (_thresh_robust)', '<A> (_thresh_robust_pca)'\n",
    "   ]].mean(axis=1)\n",
    "\n",
    "sorted_df = df.sort_values(by='mean', ascending=False).drop(['mean'], axis=1)\n",
    "\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840a620",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2dce4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sorted_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d302a1-ca18-4870-9f97-047dfdf26b12",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# RF the best, QDA the worst - but overall very similar\n",
    "\n",
    "# Since the methods are all similar, let's look at the \"simplest\" pipelines that produce interpretable results.\n",
    "# 1. LogReg (_thresh) as a baseline\n",
    "# 2. LogReg (_thresh_standard_pca) for comparison to see \"simplification\" - may be able to visualize if dims is low enough\n",
    "# 3. DTree (_thresh) usually very good and similar to RF but easier to intepret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9161e3-a66b-4842-ab91-e13ed137d36b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. LogReg (_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d07600-4a93-4ade-b5f5-df60dde07977",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import imblearn\n",
    "\n",
    "steps, param_grid = preprocessing_factory(\n",
    "    balance=False, # Use class balancing internally\n",
    "    savgol=False,\n",
    "    threshold=True,\n",
    "    scaling=None,\n",
    "    dim_red=None\n",
    ")\n",
    "\n",
    "steps += [('model',\n",
    "           LogisticRegression(\n",
    "               random_state=42,\n",
    "               solver='lbfgs', # For speed\n",
    "               max_iter=1000,\n",
    "               multi_class='multinomial',\n",
    "               class_weight='balanced', \n",
    "               fit_intercept=True)\n",
    "          )]\n",
    "\n",
    "param_grid.update({\n",
    "    'model__penalty':['none', 'l2'],\n",
    "    'model__C':np.logspace(-3, 2, 6),\n",
    "})\n",
    "\n",
    "lr1_pipe = imblearn.pipeline.Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a7f82-6bb4-40cd-9281-021ca831653c",
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs = final_fit(X_tr, y_tr, lr1_pipe, param_grid, k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b291c-725a-483b-b308-3dc133861913",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_ # Minimal thresholding used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb9ae1-6b7e-4911-bb66-f7def607fcd9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20,8))\n",
    "\n",
    "# See the documentation for interpretation of the sign of these values:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# Positive values correspond to \"True\" (belongs to class).\n",
    "max_peaks = {}\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(gs.best_estimator_.named_steps['model'].classes_):\n",
    "        # Non-dimensionalize coefficients\n",
    "        coefs = gs.best_estimator_.named_steps['model'].coef_[i] * gs.best_estimator_.named_steps['variance_threshold'].transform(np.std(X_use, axis=0).reshape(1,-1))\n",
    "        ax.plot(\n",
    "            aligned_centers,\n",
    "            gs.best_estimator_.named_steps['variance_threshold'].inverse_transform(coefs.reshape(1,-1))[0]\n",
    "        )\n",
    "        ax.set_xlabel('Energy (keV)', fontsize=20)\n",
    "        ax.set_ylabel(r'$a_k~\\times~s$', fontsize=20)\n",
    "        max_peaks[gs.best_estimator_.named_steps['model'].classes_[i]] = np.argsort(np.abs(gs.best_estimator_.named_steps['variance_threshold'].inverse_transform(coefs.reshape(1,-1))[0]))[::-1]\n",
    "        ax.set_title('{}'.format(gs.best_estimator_.named_steps['model'].classes_[i]), fontsize=22)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    else:\n",
    "        ax.set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('lr_coef.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea57540-605a-4a12-9e65-a135c552035b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The largest peak is at 2224 keV for most\n",
    "# Qualitatively similar conclusion even if we do not account for the standard deviation or if we used mean for the scale instead of std. dev.\n",
    "for k in max_peaks:\n",
    "    print(k, aligned_centers[max_peaks[k][0]], aligned_centers[max_peaks[k][1]], aligned_centers[max_peaks[k][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f5109",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 14\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "for i, ax in zip([5, 0, 9], axes.flatten()):\n",
    "    if i < len(gs.best_estimator_.named_steps['model'].classes_):\n",
    "        coefs = gs.best_estimator_.named_steps['model'].coef_[i] * gs.best_estimator_.named_steps['variance_threshold'].transform(np.std(X_use, axis=0).reshape(1,-1))\n",
    "        ax.plot(\n",
    "            aligned_centers,\n",
    "            gs.best_estimator_.named_steps['variance_threshold'].inverse_transform(coefs.reshape(1,-1))[0],\n",
    "            color=plt.cm.tab20(i)\n",
    "        )\n",
    "        ax.set_xlabel('Energy (keV)', fontsize=14)\n",
    "        ax.set_ylabel(r'$a_k~\\times~s$', fontsize=14)\n",
    "        \n",
    "        ax.set_title('{}'.format(\n",
    "            gs.best_estimator_.named_steps['model'].classes_[i]),\n",
    "                    fontsize=16)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "mpl.rcParams['font.size'] = 12\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('lr_importances.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f23c2-cce0-4907-a11b-a7b99e4af1f5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. LogReg (_thresh_standard_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a2800-a208-47a1-8ef9-438201aad538",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import imblearn\n",
    "\n",
    "steps, param_grid = preprocessing_factory(\n",
    "    balance=False, # Use class balancing internally\n",
    "    savgol=False,\n",
    "    threshold=True,\n",
    "    scaling='standard',\n",
    "    dim_red='pca'\n",
    ")\n",
    "\n",
    "steps += [('model',\n",
    "           LogisticRegression(\n",
    "               random_state=42,\n",
    "               solver='lbfgs', # For speed\n",
    "               max_iter=1000,\n",
    "               multi_class='multinomial',\n",
    "               class_weight='balanced', \n",
    "               fit_intercept=True)\n",
    "          )]\n",
    "\n",
    "param_grid.update({\n",
    "    'model__penalty':['none', 'l2'],\n",
    "    'model__C':np.logspace(-3, 2, 6),\n",
    "    'dim_red__n_components':[2], # Force 2D for easy visualization\n",
    "})\n",
    "\n",
    "lr2_pipe = imblearn.pipeline.Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e6494-f9ca-4428-b0ea-b589dadcdae9",
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs = final_fit(X_tr, y_tr, lr2_pipe, param_grid, k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fb5a3-f387-49db-ad7f-84886cd56f73",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_ \n",
    "# Also, we are using Pareto scaling which leaves behind some \"scale\" information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99d6fc-0eb9-4a07-8631-b61ddc71d325",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.score(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3883414-eaf5-44d8-8464-b2ff62d2bb39",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95573ae-3ffa-445a-8f2c-2fb6bbc0466a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preprocess_steps = gs.best_estimator_.steps[:-1]\n",
    "preprocess_pipe = imblearn.pipeline.Pipeline(steps=preprocess_steps)\n",
    "X_proj = preprocess_pipe.transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f77b2-3b82-4cac-af0c-df1b97595834",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_hard_decision_boundaries_2d(\n",
    "    preprocess_pipe.transform(X_tr), \n",
    "    y_tr, \n",
    "    gs.best_estimator_.named_steps['model'],\n",
    "    resolution=0.0005,\n",
    "    bounds=200\n",
    ")\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.xlabel('PC 1', fontsize=16)\n",
    "plt.ylabel('PC 2', fontsize=16)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('pca_lr.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1699dc-0d12-4c7c-abd7-a6d6117e3baa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,2))\n",
    "ax.set_xlabel('Energy (keV)', fontsize=16)\n",
    "ax.set_ylabel('|Coefficient|', fontsize=16)\n",
    "ax.set_title('PC 1', fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "print(plot_importance(gs, aligned_centers, dim=0, return_max=1, ax=ax))\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('lr_pc1.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce527e8-e357-4c7d-9c00-fd11ca6fdc9f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,2))\n",
    "ax.set_xlabel('Energy (keV)', fontsize=16)\n",
    "ax.set_ylabel('|Coefficient|', fontsize=16)\n",
    "ax.set_title('PC 2', fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "print(plot_importance(gs, aligned_centers, dim=1, return_max=4, ax=ax))\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('lr_pc2.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c79ce-d0dd-452e-8e77-2d4a5cea99b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# These peaks from PCA are the same as from LR!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8699b81-a2a5-4c3b-bcab-feb2457bb160",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Soft PLS-DA (Intermediate Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa8183-cfe1-4ca5-beb2-146b5f8ba93d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = pickle.load(open('model_performances/soft_plsda_summary.pkl', 'rb'))\n",
    "\n",
    "for k,v in res.items():\n",
    "    print(k, '%.3f'%np.mean(v), '%.3f'%np.std(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cbae04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rename(name):\n",
    "    parts = name.split('_')\n",
    "    model_ = parts[0].capitalize()+' PLS-DA'\n",
    "    pre_ = ''\n",
    "    for i in range(2, len(parts)):\n",
    "        add_ = parts[i]\n",
    "        if add_ == 'standard':\n",
    "            add_ = 'Standard Scaling'\n",
    "        elif add_ == 'robust':\n",
    "            add_ = 'Robust Scaling'\n",
    "        elif add_ == 'thresh':\n",
    "            add_ = 'Thresholding'\n",
    "        elif add_ == 'pca':\n",
    "            add_ = 'PCA'\n",
    "        pre_ += add_ + ' : '\n",
    "    pre_ += model_\n",
    "    return pre_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e7283",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "renamed_res = {}\n",
    "for k in res.keys():\n",
    "    renamed_res[rename(k)] = res[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0dfc0-e065-4be6-91cd-51fd8d7ab993",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# These differences are not significant\n",
    "from pychemauth import analysis\n",
    "from pychemauth.analysis import compare\n",
    "ax_ = compare.Compare.visualize(renamed_res, n_repeats=5, alpha=0.05)\n",
    "ax_.set_title(r'TEFF +/- $\\sigma$', fontsize=16)\n",
    "ax_.legend(bbox_to_anchor=(1,-0.1), fontsize=12)\n",
    "ax_.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b79c89-6086-41e9-9cfa-6f9dc9e6a4e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No statistical difference so lets select some simple ones to examine\n",
    "# 1. _thresh (no preprocessing)\n",
    "# 2. _thresh_pca (best performer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb879fc9-634f-40e9-9e95-451abdd08360",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. PLS-DA (_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f6d2f-6116-4720-b4d9-4ba99524ace9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from pychemauth.classifier.plsda import PLSDA\n",
    "\n",
    "steps, param_grid = preprocessing_factory(\n",
    "    balance=True,\n",
    "    savgol=False,\n",
    "    threshold=True,\n",
    "    scaling=None,\n",
    "    dim_red=None\n",
    ")\n",
    "\n",
    "steps += [('model', \n",
    "           PLSDA(\n",
    "               gamma=0.01, \n",
    "               not_assigned='UNKNOWN', \n",
    "               style='soft', \n",
    "               score_metric='TEFF'))\n",
    "         ]\n",
    "\n",
    "param_grid.update({\n",
    "    'model__alpha':[0.01, 0.05],\n",
    "    'model__scale_x':[True, False],\n",
    "    'model__n_components':[1, 2, 3, 4, 5, 10, 20, 50],\n",
    "})\n",
    "\n",
    "soft_plsda_pipe = imblearn.pipeline.Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7aed60-ba84-4267-99b9-a15effec0141",
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs = final_fit(X_tr, \n",
    "               y_tr, \n",
    "               soft_plsda_pipe, \n",
    "               param_grid, \n",
    "               k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4358f-b14a-4744-be4d-b83eeccd318a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f291f5-7c2a-480a-989e-eb283d1aaf8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.score(X_tr, y_tr) # TEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c62be-fc74-4de4-a070-9e475c0b00d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.score(X_te, y_te) # TEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac026b99-264c-4123-b834-b353da768056",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df, I, CSNS, CSPS, CEFF, TSNS, TSPS, TEFF = gs.best_estimator_.named_steps['model'].figures_of_merit(\n",
    "    gs.predict(X_te),\n",
    "    y_te\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4a872",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5269c47",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TSNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66609cdc-f16d-4740-a2af-a27afdf241c3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a54c6-0c39-4d05-ab46-8aab7630d53b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perf = pd.concat([CSPS, CSNS, CEFF], axis=1)\n",
    "perf.columns = ['Test CSPS', 'Test CSNS', 'Test CEFF']\n",
    "perf['Training Counts'] = [dict(zip(*np.unique(y_tr, return_counts=True)))[i] for i in perf.index]\n",
    "perf['Test Counts'] = [dict(zip(*np.unique(y_te, return_counts=True)))[i] for i in perf.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744aeea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 3)\n",
    "perf.sort_values(by='Training Counts', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0289e-24d8-4827-a106-9815610b9bc7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PLS-DA (_thresh_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eccc9b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Very similar results obtained with _thresh_standard_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282e01e-3206-41ec-be5b-6926bf6a2606",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from pychemauth.classifier.plsda import PLSDA\n",
    "\n",
    "steps, param_grid = preprocessing_factory(\n",
    "    balance=True,\n",
    "    savgol=False,\n",
    "    threshold=True,\n",
    "    scaling=None, # 'standard',\n",
    "    dim_red='pca' # PCA automatically centers the data but does not do additional scaling\n",
    ")\n",
    "\n",
    "steps += [('model', \n",
    "           PLSDA(\n",
    "               gamma=0.01, \n",
    "               not_assigned='UNKNOWN', \n",
    "               style='soft', \n",
    "               score_metric='TEFF'))\n",
    "         ]\n",
    "\n",
    "param_grid.update({\n",
    "    'model__alpha':[0.01, 0.05],\n",
    "    'model__scale_x':[True, False],\n",
    "    'model__n_components':[1, 2, 3, 4, 5, 10, 20, 50],\n",
    "\n",
    "})\n",
    "\n",
    "soft_plsda_pipe = imblearn.pipeline.Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c0b9c-8235-477b-8aad-e1b590778338",
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs = final_fit(X_tr, y_tr, soft_plsda_pipe, param_grid, k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944af025",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1da80c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50106d1c-2c29-46db-9351-1802bc2014f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.score(X_tr, y_tr) # TEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d350341-dfc8-446d-a16c-29b05ef3f8f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.score(X_te, y_te) # TEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99df4b9-d1bd-4d3c-bb82-9a213157d3f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df, I, CSNS, CSPS, CEFF, TSNS, TSPS, TEFF = gs.best_estimator_.named_steps['model'].figures_of_merit(\n",
    "    gs.predict(X_te),\n",
    "    y_te\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27c9d0-bdf1-40d0-bc1c-c72e7928142a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdf1f0-326e-4bc2-8919-9d9e8e7b4380",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perf3 = pd.concat([CSPS, CSNS, CEFF], axis=1)\n",
    "perf3.columns = ['Test CSPS', 'Test CSNS', 'Test CEFF']\n",
    "perf3['Training Counts'] = [dict(zip(*np.unique(y_tr, return_counts=True)))[i] for i in perf3.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e24eb-f2aa-47c4-8bc6-5e53738270b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perf3.sort_values(by='Training Counts', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305dbbf-2ffe-42ec-9ac7-8d1ef94f68e4",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SIMCA (Class Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ad61d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_simca(simca_models, target_class, X_tr, y_tr, ax=None, no_legend=True):\n",
    "    preprocess_pipe = imblearn.pipeline.Pipeline(steps=simca_models[target_class].best_estimator_.steps[:-2]) # ScaledSMOTEENN should pass through anyway\n",
    "    \n",
    "    ax = simca_models[target_class].best_estimator_.named_steps['model'].model.visualize(\n",
    "        preprocess_pipe.transform(X_tr), \n",
    "        y_tr,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title('DD-SIMCA Model for \\n{}'.format(target_class))\n",
    "    if no_legend:\n",
    "        ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf18cfb",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compliant, Non-robust Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70eaf0d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = pickle.load(open('model_performances/simca_compliant_summary.pkl', 'rb'))\n",
    "\n",
    "for k,v in res.items():\n",
    "    print(k, '%.3f'%np.mean(v), '%.3f'%np.std(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22193544",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pychemauth.classifier.simca import SIMCA_Classifier\n",
    "import imblearn\n",
    "import tqdm \n",
    "\n",
    "simca_models_nr = {}\n",
    "for i, target_class in tqdm.tqdm(enumerate(sorted(np.unique(y_use)))):\n",
    "    steps, param_grid = preprocessing_factory(\n",
    "        balance=True,\n",
    "        savgol=False,\n",
    "        threshold=True,\n",
    "        scaling=None,\n",
    "        dim_red=None\n",
    "    )\n",
    "\n",
    "    steps += [('model', \n",
    "               SIMCA_Classifier(\n",
    "                   target_class=target_class, \n",
    "                   style=\"dd-simca\", \n",
    "                   use=\"compliant\",\n",
    "                   robust=None,\n",
    "                   sft=False)\n",
    "              )]\n",
    "    \n",
    "    param_grid.update({\n",
    "        'model__scale_x':[True, False],\n",
    "        'model__n_components':np.arange(1, 10+1),\n",
    "        'model__alpha':[0.01, 0.05],\n",
    "        'variance_threshold__threshold':[1.0e-8, 1.0e-6, 1.0e-4] # Force this to be high enough to essentially ignore high E bins\n",
    "    })\n",
    "    \n",
    "    simca_pipe = imblearn.pipeline.Pipeline(steps=steps)\n",
    "    simca_models_nr[target_class] = final_fit(X_tr, y_tr, simca_pipe, param_grid, k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e154f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# These performance metrics do not change, even when lower thresholds are allowed!\n",
    "simca_metrics_nr = {}\n",
    "display = []\n",
    "for target_class in simca_models_nr.keys():\n",
    "    preprocess_pipe = imblearn.pipeline.Pipeline(steps=simca_models_nr[target_class].best_estimator_.steps[:-2]) # ScaledSMOTEENN should pass through anyway\n",
    "    \n",
    "    simca_metrics_nr[target_class] = simca_models_nr[target_class].best_estimator_.named_steps['model'].metrics(\n",
    "        preprocess_pipe.transform(X_te), \n",
    "        y_te\n",
    "    )\n",
    "    \n",
    "    print(target_class, simca_models_nr[target_class].best_params_['model__alpha'])\n",
    "    \n",
    "    display.append([\n",
    "        target_class, \n",
    "        simca_metrics_nr[target_class]['tsns'], \n",
    "        simca_metrics_nr[target_class]['tsps'], \n",
    "        simca_metrics_nr[target_class]['teff'], \n",
    "        simca_models_nr[target_class].best_params_['model__n_components'], \n",
    "        simca_models_nr[target_class].best_params_['variance_threshold__threshold'], \n",
    "        simca_models_nr[target_class].best_params_['model__alpha']\n",
    "    ])\n",
    "simca_perf_nr = pd.DataFrame(data=display, columns=['Target class', 'Test TSNS', 'Test TSPS', 'Test TEFF', 'n_components', 'threshold', 'alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ea2dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 3)\n",
    "simca_perf_nr['Target class'] = [x_ for x_ in simca_perf_nr['Target class']]\n",
    "simca_perf_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68282c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15,10))\n",
    "for ax, target_class in zip(axes.flatten(), simca_models_nr.keys()):\n",
    "    plot_simca(simca_models_nr, target_class, X_tr, y_tr, ax=ax)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.set_title(ax.get_title(), fontsize=18)\n",
    "    ax.set_xlabel('ln'+'(1 + h/h'+r'$_0$'+')', fontsize=16)\n",
    "    ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=16)\n",
    "\n",
    "axes.flatten()[-1].set_visible(False)\n",
    "axes.flatten()[-2].set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('simca_results_nr.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414eedb8-752b-4469-8fb5-b13b5b8c6e5a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rigorous, Non-robust Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ab19a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = pickle.load(open('model_performances/simca_rigorous_summary.pkl', 'rb'))\n",
    "\n",
    "for k,v in res.items():\n",
    "    print(k, '%.3f'%np.mean(v), '%.3f'%np.std(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1566c-a77f-4ef6-8138-4a6eb1dd8ec5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Rigorous models seek to get TSNS ~ 1-alpha, and the score is reported as -(TSNS - (1-alpha))^2\n",
    "# These are much \"worse\", but remember that alpha was fixed to 0.05 whereas the others optimized to get to 0.01 instead.\n",
    "\n",
    "res = pickle.load(open('model_performances/simca_rigorous_summary.pkl', 'rb'))\n",
    "\n",
    "for k,v in res.items():\n",
    "    print(k, '%.3f'%np.mean(v), '%.3f'%np.std(v), '; <deviation> from 0.95 = {}'.format('%.3f'%np.sqrt(-np.mean(v))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f678d-7434-4105-a03f-1fcabe94aa6f",
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pychemauth.classifier.simca import SIMCA_Classifier\n",
    "import imblearn\n",
    "import tqdm \n",
    "\n",
    "simca_models_rig = {}\n",
    "for i, target_class in tqdm.tqdm(enumerate(sorted(np.unique(y_use)))):\n",
    "    steps, param_grid = preprocessing_factory(\n",
    "        balance=True,\n",
    "        savgol=False,\n",
    "        threshold=True,\n",
    "        scaling=None,\n",
    "        dim_red=None\n",
    "    )\n",
    "\n",
    "    steps += [('model', \n",
    "               SIMCA_Classifier(\n",
    "                   target_class=target_class, \n",
    "                   style=\"dd-simca\", \n",
    "                   use=\"rigorous\",\n",
    "                   robust=None, # It was better to use classical estimators instead of robust ones\n",
    "                   alpha=0.01, # 0.01 was universally selected by the compliant models above, so use this value for fairer comparison\n",
    "                   sft=False)\n",
    "              )]\n",
    "    \n",
    "    param_grid.update({\n",
    "        'model__scale_x':[True, False],\n",
    "        'model__n_components':np.arange(1, 10+1),\n",
    "        'variance_threshold__threshold':[1.0e-8, 1.0e-6, 1.0e-4] # Force this to be high enough to essentially ignore high E bins - also same as compliant models above\n",
    "    })\n",
    "    \n",
    "    simca_pipe = imblearn.pipeline.Pipeline(steps=steps)\n",
    "    try:\n",
    "        simca_models_rig[target_class] = final_fit(\n",
    "            X_tr,  \n",
    "            y_tr, \n",
    "            simca_pipe, param_grid, k_fold)\n",
    "    except:\n",
    "        print('1.0e-4 bad for : '+target_class)\n",
    "        param_grid.update({\n",
    "            'variance_threshold__threshold':[1.0e-8, 1.0e-6] # Force this to be high enough to essentially ignore high E bins - also same as compliant models above\n",
    "        })\n",
    "        \n",
    "        simca_models_rig[target_class] = final_fit(\n",
    "            X_tr,  \n",
    "            y_tr, \n",
    "            simca_pipe, param_grid, k_fold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fddc3c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simca_metrics_rig = {}\n",
    "display = []\n",
    "for target_class in simca_models_rig.keys():\n",
    "    preprocess_pipe = imblearn.pipeline.Pipeline(steps=simca_models_rig[target_class].best_estimator_.steps[:-2]) # ScaledSMOTEENN should pass through anyway\n",
    "    \n",
    "    simca_metrics_rig[target_class] = simca_models_rig[target_class].best_estimator_.named_steps['model'].metrics(\n",
    "        preprocess_pipe.transform(X_te), \n",
    "        y_te\n",
    "    )\n",
    "    \n",
    "    display.append([target_class, \n",
    "                    simca_metrics_rig[target_class]['tsns'], \n",
    "                    simca_metrics_rig[target_class]['tsps'], \n",
    "                    simca_metrics_rig[target_class]['teff'], \n",
    "                    simca_models_rig[target_class].best_params_['model__n_components'], \n",
    "                    simca_models_rig[target_class].best_params_['variance_threshold__threshold']\n",
    "                   ])\n",
    "                    \n",
    "simca_df = pd.DataFrame(data=display, columns=['Target class', 'Test TSNS', 'Test TSPS', 'Test TEFF', 'n_components', \n",
    "                                    'threshold'\n",
    "                                   ])\n",
    "simca_df['Training Counts'] = [dict(zip(*np.unique(y_tr, return_counts=True)))[i] for i in simca_df['Target class']]\n",
    "\n",
    "simca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b020a36",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15,10))\n",
    "for ax, target_class in zip(axes.flatten(), simca_models_rig.keys()):\n",
    "    plot_simca(simca_models_rig, target_class, X_tr, y_tr, ax=ax)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.set_title(ax.get_title(), fontsize=18)\n",
    "    ax.set_xlabel('ln'+'(1 + h/h'+r'$_0$'+')', fontsize=16)\n",
    "    ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=16)\n",
    "axes.flatten()[-1].set_visible(False)\n",
    "axes.flatten()[-2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('simca_results_rig.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9805f",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look at PC 1 for these models\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10,10))\n",
    "\n",
    "for ax, target_class in zip(axes.flatten(), simca_models_rig.keys()):\n",
    "    ax.set_title(target_class)\n",
    "\n",
    "    scale = simca_models_rig[target_class].best_estimator_.named_steps['model'].model._DDSIMCA_Model__ss_._CorrectedScaler__std_\n",
    "    loadings = simca_models_rig[target_class].best_estimator_.named_steps['model'].model._DDSIMCA_Model__pca_.components_[0]\n",
    "    if simca_models_rig[target_class].best_params_['model__scale_x']:\n",
    "        loadings *= scale # If X is divided by std, multiply back to make a fair comparison with other models\n",
    "        add=r'$~\\times~\\sigma$'\n",
    "    else:\n",
    "        add=''\n",
    "    y_ = simca_models_rig[target_class].best_estimator_.named_steps['variance_threshold'].inverse_transform(loadings.reshape(1,-1))[0]\n",
    "    \n",
    "    ax.plot(aligned_centers, y_)\n",
    "    ax.ticklabel_format(useOffset=False, useMathText=True, scilimits=(0,0), axis='y')\n",
    "    \n",
    "    ax.set_xlabel('Energy (keV)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db01fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=False, facecolor='white', figsize=(18,8))\n",
    "gs = fig.add_gridspec(nrows=4, ncols=4, left=0.05, right=0.75,\n",
    "                      hspace=0.9, wspace=0.3)\n",
    "\n",
    "for i,target_class in enumerate(\n",
    "    ['Fuel Oil', 'Concrete', 'Forensic Glass', 'Titanium Alloy']\n",
    "):\n",
    "    ax = fig.add_subplot(gs[i, :2])\n",
    "    \n",
    "    ax.set_title('PC 1 for '+target_class, fontsize=18)\n",
    "\n",
    "    scale = simca_models_rig[target_class].best_estimator_.named_steps['model'].model._DDSIMCA_Model__ss_._CorrectedScaler__std_\n",
    "    scalings = np.abs(simca_models_rig[target_class].best_estimator_.named_steps['model'].model._DDSIMCA_Model__pca_.components_[0])\n",
    "    if simca_models_rig[target_class].best_params_['model__scale_x']:\n",
    "        scalings *= scale # If X is divided by std, multiply back to make a fair comparison with other models\n",
    "        add=r'$~\\times~s$'\n",
    "    else:\n",
    "        add=''\n",
    "    y_ = simca_models_rig[target_class].best_estimator_.named_steps['variance_threshold'].inverse_transform(scalings.reshape(1,-1))[0]\n",
    "    \n",
    "    print(target_class, aligned_centers[np.argsort(y_)[::-1][0]], aligned_centers[np.argsort(y_)[::-1][1]])\n",
    "    \n",
    "    ax.plot(aligned_centers, y_)\n",
    "    ax.ticklabel_format(useOffset=False, useMathText=True, scilimits=(0,0), axis='y')\n",
    "    \n",
    "    ax.set_xlabel('Energy (keV)', fontsize=14)\n",
    "    ax.set_ylabel('|Coefficient|'+add, fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "ax = fig.add_subplot(gs[:2, 2])\n",
    "plot_simca(simca_models_rig, 'Fuel Oil', X_tr, y_tr, ax=ax)\n",
    "ax.set_title('Rigorous DD-SIMCA \\nModel for Fuel Oil', fontsize=18)\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=15)\n",
    "\n",
    "ax = fig.add_subplot(gs[:2, 3])\n",
    "plot_simca(simca_models_rig, 'Concrete', X_tr, y_tr, ax=ax)\n",
    "ax.set_title('Rigorous DD-SIMCA \\nModel for Concrete', fontsize=18)\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=15)\n",
    "\n",
    "ax = fig.add_subplot(gs[2:, 2])\n",
    "plot_simca(simca_models_rig, 'Forensic Glass', X_tr, y_tr, ax=ax)\n",
    "ax.set_title('Rigorous DD-SIMCA \\nModel for Forensic Glass', fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=15)\n",
    "ax.set_xlabel('ln'+'(1 + h/h'+r'$_0$'+')', fontsize=15)\n",
    "\n",
    "ax = fig.add_subplot(gs[2:, 3])\n",
    "plot_simca(simca_models_rig, 'Titanium Alloy', X_tr, y_tr, ax=ax)\n",
    "ax.set_title('Rigorous DD-SIMCA \\nModel for Titanium Alloy', fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_xlabel('ln'+'(1 + h/h'+r'$_0$'+')', fontsize=15)\n",
    "ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=15)\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('ddsimca.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fcf587",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare Coal and Coke Rigorous vs. Compliant\n",
    "# Training data shown to illustrate how the models are constructed.\n",
    "target_class = 'Coal and Coke'    \n",
    "\n",
    "fig = plt.figure(constrained_layout=False, facecolor='white', figsize=(12,4))\n",
    "gs = fig.add_gridspec(nrows=1, ncols=3, left=0.05, right=0.95,\n",
    "                      hspace=0.9, wspace=0.3)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "preprocess_pipe = imblearn.pipeline.Pipeline(steps=simca_models_nr[target_class].best_estimator_.steps[:-2]) \n",
    "simca_models_nr[target_class].best_estimator_.named_steps['model'].model.visualize(\n",
    "    preprocess_pipe.transform(X_tr), \n",
    "    y_tr,\n",
    "    ax=ax\n",
    ")\n",
    "_ = ax.set_title('Compliant DD-SIMCA \\nModel for '+target_class, fontsize=18)\n",
    "ax.legend(bbox_to_anchor=(1,1.03), fontsize=12)    \n",
    "ax.set_xlabel('ln'+'(1 + h/h'+r'$_0$'+')', fontsize=16)\n",
    "ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "preprocess_pipe = imblearn.pipeline.Pipeline(steps=simca_models_rig[target_class].best_estimator_.steps[:-2]) \n",
    "simca_models_rig[target_class].best_estimator_.named_steps['model'].model.visualize(\n",
    "    preprocess_pipe.transform(X_tr), \n",
    "    y_tr,\n",
    "    ax=ax\n",
    ")\n",
    "_ = ax.set_title('Rigorous DD-SIMCA \\nModel for '+target_class, fontsize=18)\n",
    "ax.legend(bbox_to_anchor=(1,1.03), fontsize=12)\n",
    "ax.set_xlabel('ln'+'(1 + h/h'+r'$_0$'+')', fontsize=16)\n",
    "ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('rig_vs_comp_ddsimca.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4104e-6c0b-44aa-9d66-9b7bb7dbe639",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Testing Authentication Models against Other Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d30ee-7935-4829-91f4-d74c9bfdaeda",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.unique(y_challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a00f66",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "use_simca = simca_models_rig\n",
    "# simca_models_rig (rigorous, non-robust)\n",
    "# simca_models_nr (compliant, non-robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee71ad-1669-4cf9-8060-d74166b1d028",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simca_challenge_metrics = {}\n",
    "display = []\n",
    "for target_class in use_simca.keys():\n",
    "    preprocess_pipe = imblearn.pipeline.Pipeline(steps=use_simca[target_class].best_estimator_.steps[:-2]) # ScaledSMOTEENN should pass through anyway\n",
    "    \n",
    "    simca_challenge_metrics[target_class] = use_simca[target_class].best_estimator_.named_steps['model'].metrics(\n",
    "        preprocess_pipe.transform(X_challenge), \n",
    "        y_challenge\n",
    "    )\n",
    "    \n",
    "    display.append([target_class, \n",
    "                    simca_challenge_metrics[target_class]['tsns'], \n",
    "                    simca_challenge_metrics[target_class]['tsps'], \n",
    "                    simca_challenge_metrics[target_class]['teff'], \n",
    "                    use_simca[target_class].best_params_['model__n_components'], \n",
    "                    use_simca[target_class].best_params_['variance_threshold__threshold'], \n",
    "                   ])\n",
    "df_chall = pd.DataFrame(data=display, columns=['Target class', 'Test TSNS', 'Test TSPS', 'Test TEFF', 'n_components', 'threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ca9f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_chall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8750434-9409-4e9c-9c33-45ed4a0ed504",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Perfect performance from both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4e6f2-eee3-47dc-a748-6d561378af18",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for target_class in use_simca.keys():\n",
    "    preprocess_pipe = imblearn.pipeline.Pipeline(steps=use_simca[target_class].best_estimator_.steps[:-2]) \n",
    "    \n",
    "    use_simca[target_class].best_estimator_.named_steps['model'].model.visualize(\n",
    "        preprocess_pipe.transform(X_challenge), \n",
    "        y_challenge\n",
    "    )\n",
    "    plt.gca().set_title('DD-SIMCA Model for: {}'.format(target_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acdc53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18,9))\n",
    "\n",
    "plot_simca(use_simca, 'Fuel Oil', X_challenge, y_challenge, ax=axes[0][0], no_legend=True)\n",
    "plot_simca(use_simca, 'Concrete', X_challenge, y_challenge, ax=axes[0][1], no_legend=True)\n",
    "plot_simca(use_simca, 'Forensic Glass', X_challenge, y_challenge, ax=axes[0][2], no_legend=True)\n",
    "plot_simca(use_simca, 'Titanium Alloy', X_challenge, y_challenge, ax=axes[1][0], no_legend=True)\n",
    "plot_simca(use_simca, 'Biomass', X_challenge, y_challenge, ax=axes[1][1], no_legend=True)\n",
    "plot_simca(use_simca, 'Coal and Coke', X_challenge, y_challenge, ax=axes[1][2], no_legend=False)\n",
    "\n",
    "\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1, 2]:\n",
    "        axes[i][j].xaxis.label.set_size(14)\n",
    "        axes[i][j].yaxis.label.set_size(14)\n",
    "        axes[i][j].title.set_size(14)\n",
    "        axes[i][j].tick_params(axis='x', labelsize=12)\n",
    "        axes[i][j].tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "axes[0][0].legend(loc='upper left', fontsize=10)\n",
    "axes[0][1].legend(loc='upper left', fontsize=10)\n",
    "axes[0][2].legend(loc='upper right', fontsize=10)\n",
    "axes[1][0].legend(loc='upper right', fontsize=10)\n",
    "axes[1][1].legend(loc='lower right', fontsize=10)\n",
    "axes[1][2].legend(loc='upper left', fontsize=10)\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.15,\n",
    "                    hspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bbfde",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_class = 'Coal and Coke'    \n",
    "\n",
    "fig = plt.figure(constrained_layout=False, facecolor='white', figsize=(12,4))\n",
    "gs = fig.add_gridspec(nrows=1, ncols=3, left=0.05, right=0.95,\n",
    "                      hspace=0.9, wspace=0.3)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "preprocess_pipe = imblearn.pipeline.Pipeline(steps=simca_models_nr[target_class].best_estimator_.steps[:-2]) \n",
    "simca_models_nr[target_class].best_estimator_.named_steps['model'].model.visualize(\n",
    "    preprocess_pipe.transform(X_challenge), \n",
    "    y_challenge,\n",
    "    ax=ax\n",
    ")\n",
    "_ = ax.set_title('Compliant DD-SIMCA \\nModel for '+target_class, fontsize=18)\n",
    "ax.legend(bbox_to_anchor=(1,1.03), fontsize=14)\n",
    "ax.set_xlabel('ln'+'(1 + h/h'+r'$_0$'+')', fontsize=16)\n",
    "ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "preprocess_pipe = imblearn.pipeline.Pipeline(steps=simca_models_rig[target_class].best_estimator_.steps[:-2]) \n",
    "simca_models_rig[target_class].best_estimator_.named_steps['model'].model.visualize(\n",
    "    preprocess_pipe.transform(X_challenge), \n",
    "    y_challenge,\n",
    "    ax=ax\n",
    ")\n",
    "_ = ax.set_title('Rigorous DD-SIMCA \\nModel for '+target_class, fontsize=18)\n",
    "ax.legend(bbox_to_anchor=(1,1.03), fontsize=14)\n",
    "ax.set_xlabel('ln'+'(1 + h/h'+r'$_0$'+')', fontsize=16)\n",
    "ax.set_ylabel('ln'+'(1 + q/q'+r'$_0$'+')', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "if new_figures:\n",
    "    plt.savefig('auth_ddsimca.png', dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgaa",
   "language": "python",
   "name": "pgaa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
